{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrow\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from salishsea_tools import rivertools, nc_tools, viz_tools\n",
    "from salishsea_tools import river_202101 as rivers\n",
    "\n",
    "prop_dict_name ='river_202101'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what coordinates are you using?\n",
    "gridcoords = 'coordinates_seagrid_SalishSea201702.nc'\n",
    "coords_file = '../../../grid/'+gridcoords\n",
    "# where is the river information? \n",
    "prop_dict = rivers.prop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dimensions for netcdf files\n",
    "fB = xr.open_dataset(coords_file, decode_times=False)\n",
    "lat = fB['nav_lat'][:]\n",
    "lon = fB['nav_lon'][:]\n",
    "e1t = fB['e1t'][0,:]\n",
    "e2t = fB['e2t'][0,:]\n",
    "horz_area = e1t*e2t\n",
    "fB.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of watersheds we are including\n",
    "names = ['bute', 'evi_n', 'jervis', 'evi_s', 'howe', 'jdf', 'skagit', 'puget', 'toba', 'fraser']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01T00:00:00+00:00 2018-12-31T00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Constant and data ranges etc\n",
    "year = 2018\n",
    "smonth = 1\n",
    "emonth = 12\n",
    "startdate = arrow.get(year, smonth, 1)\n",
    "enddate = arrow.get(year, emonth, 31)\n",
    "print (startdate, enddate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_river(river_name, ps):\n",
    "    river_flow = pd.read_csv(f'river_flows/{river_name}_flow', header=None, sep='\\s+', index_col=False,\n",
    "                      names=['year', 'month', 'day', 'flow'])\n",
    "    river_flow['date'] = pd.to_datetime(river_flow.drop(columns='flow'))\n",
    "    river_flow.set_index('date', inplace=True)\n",
    "    river_flow = river_flow.drop(columns=['year', 'month', 'day'])\n",
    "    if ps == 'primary':\n",
    "        river_flow = river_flow.rename(columns={'flow': 'Primary River Flow'})\n",
    "    elif ps == 'secondary':\n",
    "        river_flow = river_flow.rename(columns={'flow': 'Secondary River Flow'})\n",
    "    return river_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_river_Theodosia():\n",
    "    part1 = pd.read_csv('river_flows/Theodosia_Scotty_flow', header=None, sep='\\s+', index_col=False,\n",
    "                      names=['year', 'month', 'day', 'flow'])\n",
    "    part2 = pd.read_csv('river_flows/Theodosia_Bypass_flow', header=None, sep='\\s+', index_col=False,\n",
    "                      names=['year', 'month', 'day', 'flow'])\n",
    "    part3 = pd.read_csv('river_flows/Theodosia_Diversion_flow', header=None, sep='\\s+', index_col=False,\n",
    "                      names=['year', 'month', 'day', 'flow'])\n",
    "    for part in [part1, part2, part3]:\n",
    "        part['date'] = pd.to_datetime(part.drop(columns='flow'))\n",
    "        part.set_index('date', inplace=True)\n",
    "        part.drop(columns=['year', 'month', 'day'], inplace=True)\n",
    "    part1 = part1.rename(columns={'flow': 'Scotty'})\n",
    "    part2 = part2.rename(columns={'flow': 'Bypass'})\n",
    "    part3 = part3.rename(columns={'flow': 'Diversion'})\n",
    "    theodosia = (part1.merge(part2, how='inner', on='date')).merge(part3, how='inner', on='date')\n",
    "    theodosia['Secondary River Flow'] = theodosia['Scotty'] + theodosia['Diversion'] - theodosia['Bypass']\n",
    "    return theodosia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_a_pair(water_shed, watershed_from_river, startdate, enddate,\n",
    "              primary_river_name, use_secondary, \n",
    "              secondary_river_name='Null', subtract_primary=False):\n",
    "    primary_river = read_river(primary_river_name, 'primary')\n",
    "    good, fillit = check_for_gaps(primary_river_name, primary_river['Primary River Flow'], startdate, enddate)\n",
    "    if not good:\n",
    "        print ('filling', primary_river_name)\n",
    "        myfill = fillit[fillit.index > np.datetime64(dt.datetime(2006, 12, 31))]\n",
    "        fillitd = pd.DataFrame(index = myfill.index, data = {'Primary River Flow': myfill})\n",
    "        primary_river = primary_river.append(fillitd)\n",
    "        primary_river = primary_river.sort_index()\n",
    "            \n",
    "    if use_secondary:\n",
    "        if secondary_river_name == \"Theodosia\":\n",
    "            secondary_river = read_river_Theodosia()\n",
    "            \n",
    "        else:\n",
    "            secondary_river = read_river(secondary_river_name, 'secondary')\n",
    "        good, fillit = check_for_gaps(secondary_river_name, secondary_river['Secondary River Flow'], \n",
    "                                      startdate, enddate)\n",
    "        if not good:\n",
    "            print ('filling', secondary_river_name)\n",
    "            myfill = fillit[fillit.index > np.datetime64(dt.datetime(2006, 12, 31))]\n",
    "            fillitd = pd.DataFrame(index = myfill.index, data = {'Secondary River Flow': myfill})\n",
    "            secondary_river = secondary_river.append(fillitd)\n",
    "            secondary_river = secondary_river.sort_index()\n",
    "            secondary_river.index.name = 'date'\n",
    "                        \n",
    "        rivers = primary_river.merge(secondary_river, how='inner', on='date')\n",
    "        rivers['Daily Flow'] = (rivers['Primary River Flow'] * \n",
    "                        watershed_from_river[water_shed]['primary']\n",
    "                        + rivers['Secondary River Flow'] \n",
    "                        * watershed_from_river[water_shed]['secondary'])\n",
    "    else:\n",
    "        rivers = primary_river\n",
    "        rivers['Daily Flow'] = (primary_river['Primary River Flow'] * \n",
    "                                watershed_from_river[water_shed]['primary'])\n",
    "\n",
    "    \n",
    "    return rivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fraser(watershed_from_river, startdate, enddate,\n",
    "              primary_river_name, secondary_river_name):\n",
    "    primary_river = read_river(primary_river_name, 'primary')\n",
    "    good, fillit = check_for_gaps(primary_river_name, primary_river['Primary River Flow'], startdate, enddate)\n",
    "    if not good:\n",
    "        print ('filling', primary_river_name)\n",
    "        myfill = fillit[fillit.index > np.datetime64(dt.datetime(2006, 12, 31))]\n",
    "        fillitd = pd.DataFrame(index = myfill.index, data = {'Primary River Flow': myfill})\n",
    "        primary_river = primary_river.append(fillitd)\n",
    "        primary_river = primary_river.sort_index()\n",
    "            \n",
    "    secondary_river = read_river(secondary_river_name, 'secondary')\n",
    "    good, fillit = check_for_gaps(secondary_river_name, secondary_river['Secondary River Flow'], \n",
    "                                      startdate, enddate)\n",
    "    if not good:\n",
    "        print ('filling', secondary_river_name)\n",
    "        myfill = fillit[fillit.index > np.datetime64(dt.datetime(2006, 12, 31))]\n",
    "        fillitd = pd.DataFrame(index = myfill.index, data = {'Secondary River Flow': myfill})\n",
    "        secondary_river = secondary_river.append(fillitd)\n",
    "        secondary_river = secondary_river.sort_index()    \n",
    "                        \n",
    "    Fraser = primary_river.merge(secondary_river, how='inner', on='date')\n",
    "    Fraser['Daily Flow'] = (Fraser['Primary River Flow'] * \n",
    "                        watershed_from_river['fraser']['primary']\n",
    "                        + Fraser['Secondary River Flow'] \n",
    "                        * watershed_from_river['fraser']['secondary'] *\n",
    "                           watershed_from_river['fraser']['nico_into_fraser'])\n",
    "    secondary_river['Daily Flow'] = (secondary_river['Secondary River Flow'] *\n",
    "                                      watershed_from_river['fraser']['secondary'] *\n",
    "                           (1 - watershed_from_river['fraser']['nico_into_fraser']))\n",
    "\n",
    "    \n",
    "    return Fraser, secondary_river"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_gaps(name, flows, startdate, enddate):\n",
    "    subset = flows[(flows.index <= enddate.naive) & (flows.index >= startdate.naive)]\n",
    "    diffy = subset.index[1:] - subset.index[:-1]\n",
    "    day = dt.datetime(2020, 1, 2) - dt.datetime(2020, 1, 1)\n",
    "    gaps = subset[1:][diffy > day]\n",
    "    gap_length = diffy[diffy > day]\n",
    "    if len(gap_length) == 0:\n",
    "        good = True\n",
    "        fillit = pd.Series(dtype='float64')\n",
    "    else:\n",
    "        good = False\n",
    "        print ('got a gap')\n",
    "        fillvalue, filldate = patch_gaps(name, flows, gaps, gap_length, day)\n",
    "        fillit = pd.Series(fillvalue, index=filldate)\n",
    "\n",
    "    return good, fillit    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_dictionary = {'Englishman': 'Salmon_Sayward',\n",
    "                      'Theodosia': 'Englishman',\n",
    "                      'Roberts_Roberts': 'Englishman'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_gaps(name, flows, gaps, gap_length, day):\n",
    "    filler = read_river(matching_dictionary[name], 'primary')\n",
    "    fillvalue = np.zeros((len(gaps.index), gap_length.max().days))\n",
    "    filldate = np.zeros((len(gaps.index), gap_length.max().days), dtype='datetime64[ns]')\n",
    "    for ig, gap in enumerate(gaps.index):\n",
    "        summit = 0\n",
    "        sumsq = 0\n",
    "        count = 0\n",
    "        print (gap_length[ig], gap)\n",
    "        for deltat in np.arange(-2*gap_length[ig]+day, gap_length[ig], day):\n",
    "            summit = summit + filler[filler.index == gap+deltat]['Primary River Flow'].values\n",
    "            sumsq = sumsq + filler[filler.index == gap+deltat]['Primary River Flow'].values**2\n",
    "            count = count + 1\n",
    "        mean = summit/count\n",
    "        var = sumsq/count - mean**2/count\n",
    "        sumbase = 0\n",
    "        sumfill = 0\n",
    "        sumratio = 0\n",
    "        sumratio2 = 0\n",
    "        count = 0\n",
    "        for deltat in np.arange(-2*gap_length[ig]+day, -gap_length[ig]+day, day):\n",
    "            sumbase = sumbase + flows[flows.index == gap+deltat].values\n",
    "            sumfill = sumfill + filler[filler.index == gap+deltat]['Primary River Flow'].values\n",
    "            ratio = (flows[flows.index == gap+deltat].values /\n",
    "                filler[filler.index == gap+deltat]['Primary River Flow'].values)\n",
    "            sumratio = sumratio + ratio\n",
    "            sumratio2 = sumratio2 + ratio**2\n",
    "            count = count + 1\n",
    "        for deltat in np.arange(0, gap_length[ig], day):\n",
    "            sumbase = sumbase + flows[flows.index == gap+deltat].values\n",
    "            sumfill = sumfill + filler[filler.index == gap+deltat]['Primary River Flow'].values\n",
    "            ratio = (flows[flows.index == gap+deltat].values / \n",
    "                     filler[filler.index == gap+deltat]['Primary River Flow'].values)\n",
    "            sumratio = sumratio + ratio\n",
    "            sumratio2 = sumratio2 + ratio**2\n",
    "            count = count + 1\n",
    "        meanratio = sumratio/count\n",
    "        varratio = sumratio2/count - meanratio**2/count\n",
    "        if np.sqrt(var)/mean < np.sqrt(varratio)/meanratio : \n",
    "            uselinear = True\n",
    "        else:\n",
    "            uselinear = False\n",
    "        print (uselinear, 'uselinear')\n",
    "        for step, deltat in enumerate(np.arange(-gap_length[ig]+day, day, day)):\n",
    "            if uselinear:\n",
    "                slope = (flows[flows.index == gap].values \n",
    "                 - flows[flows.index == gap-gap_length[ig]].values)/gap_length[ig].days\n",
    "                fillvalue[ig, step] = flows[flows.index == gap-gap_length[ig]] + slope * (step+1)\n",
    "            else:\n",
    "                fillvalue[ig, step] = meanratio * filler[filler.index == gap + deltat].values \n",
    "            filldate[ig, step] = gap + deltat\n",
    "    return fillvalue.flatten(), filldate.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_from_river = {\n",
    "    'bute': {'primary': 2.015},\n",
    "    'jervis': {'primary': 8.810, 'secondary': 140.3},\n",
    "    'howe': {'primary': 2.276},\n",
    "    'jdf': {'primary': 8.501},\n",
    "    'evi_n': {'primary': 10.334},\n",
    "    'evi_s': {'primary': 24.60},\n",
    "    'toba': {'primary': 0.4563, 'secondary': 14.58},\n",
    "    'skagit': {'primary': 1.267, 'secondary': 1.236},\n",
    "    'puget': {'primary': 8.790, 'secondary': 29.09},\n",
    "    'fraser' : {'primary': 1.161, 'secondary': 162, 'nico_into_fraser': 0.83565}\n",
    "}\n",
    "rivers_for_watershed = {\n",
    "    'bute': {'primary': 'Homathko_Mouth',\n",
    "            'secondary': 'False'},\n",
    "    'evi_n': {'primary': 'Salmon_Sayward',\n",
    "             'secondary': 'False'},\n",
    "    'jervis': {'primary': 'Clowhom_ClowhomLake',\n",
    "              'secondary': 'Roberts_Roberts'},\n",
    "    'evi_s': {'primary': 'Englishman', \n",
    "             'secondary': 'False'},\n",
    "    'howe': {'primary': 'Squamish_Brackendale',\n",
    "            'secondary': 'False'},\n",
    "    'jdf': {'primary': 'SanJuan_PortRenfrew',\n",
    "           'secondary': 'False'},\n",
    "    'skagit': {'primary': 'Skagit_MountVernon',\n",
    "              'secondary': 'Snohomish_Monroe'},\n",
    "    'puget': {'primary': 'Nisqually_McKenna',\n",
    "             'secondary': 'Greenwater_Greenwater'},\n",
    "    'toba': {'primary': 'Homathko_Mouth',\n",
    "            'secondary': 'Theodosia'},\n",
    "    'fraser': {'primary': 'Fraser_Hope',\n",
    "              'secondary': 'Nicomekl_203'},\n",
    "}\n",
    "\n",
    "fraserratio = rivers.prop_dict['fraser']['Fraser']['prop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(day, runoff):\n",
    "    \"keep it small and simple, runoff only\"\n",
    "    notebook = 'MakeDailyRiverNCfiles.ipynb'\n",
    "    coords = {\n",
    "        'x' : range(398),\n",
    "        'y' : range(898),\n",
    "        'time_counter' : [0],\n",
    "    }\n",
    "    var_attrs = {'units': 'kg m-2 s-1',\n",
    "                      'long_name': 'runoff_flux'}\n",
    "    \n",
    "    year = day.year\n",
    "    month = day.month\n",
    "    day = day.day\n",
    "    # set up filename to follow NEMO conventions\n",
    "    filename = f'ncfiles/R202102Dailies_y{year}m{month:02}d{day:02}.nc'\n",
    "    print (filename)\n",
    "        \n",
    "    netcdf_title = f'Rivers for y{year}m{month:02}d{day:02}'\n",
    "    ds_attrs = {\n",
    "        'acknowledgements':\n",
    "            'Based on river fit',\n",
    "        'creator_email':\n",
    "            'sallen@eoas.ubc.ca',\n",
    "        'creator_name':\n",
    "            'Salish Sea MEOPAR Project Contributors',\n",
    "        'creator_url':\n",
    "            'https://salishsea-meopar-docs.readthedocs.org/',\n",
    "        'institution':\n",
    "            'UBC EOAS',\n",
    "        'institution_fullname': (\n",
    "            'Earth, Ocean & Atmospheric Sciences,'\n",
    "            ' University of British Columbia'\n",
    "        ),\n",
    "        'title': netcdf_title,\n",
    "        'notebook': notebook,\n",
    "        'rivers_base': prop_dict_name,\n",
    "        'summary': f'Daily Runoff',\n",
    "        'history': (\n",
    "            '[{}] File creation.'\n",
    "            .format(dt.datetime.today().strftime('%Y-%m-%d'))\n",
    "        )\n",
    "        }\n",
    "    runoffs = np.empty((1, runoff.shape[0], runoff.shape[1]))\n",
    "    runoffs[0] = runoff\n",
    "\n",
    "    da = xr.DataArray(\n",
    "            data = runoffs,\n",
    "            name='rorunoff',\n",
    "            dims=('time_counter', 'y', 'x'),\n",
    "            coords = coords,\n",
    "            attrs = var_attrs)\n",
    "    \n",
    "\n",
    "    ds = xr.Dataset(\n",
    "            data_vars={\n",
    "            'rorunoff': da},\n",
    "            coords = coords,\n",
    "            attrs = ds_attrs\n",
    "        )\n",
    "    \n",
    "    encoding = {var: {'zlib': True} for var in ds.data_vars}\n",
    "\n",
    "    ds.to_netcdf(filename, unlimited_dims=('time_counter'),\n",
    "            encoding=encoding,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bute\n",
      "evi_n\n",
      "jervis\n",
      "got a gap\n",
      "9 days 00:00:00 2018-06-03 00:00:00\n",
      "False uselinear\n",
      "filling Roberts_Roberts\n",
      "evi_s\n",
      "howe\n",
      "jdf\n",
      "skagit\n",
      "puget\n",
      "toba\n",
      "fraser\n",
      "            Secondary River Flow  Daily Flow\n",
      "date                                        \n",
      "2019-12-08              1.913160   50.937311\n",
      "2019-12-09              1.277257   34.006584\n",
      "2019-12-10              1.105156   29.424447\n",
      "2019-12-11              2.183681   58.139852\n",
      "2019-12-12             10.506880  279.742528\n",
      "...                          ...         ...\n",
      "2020-12-30             12.370490  329.360585\n",
      "2020-12-31             18.568590  494.383138\n",
      "2021-01-01              9.472535  252.203403\n",
      "2021-01-02              8.384792  223.242572\n",
      "2021-01-03              9.653576  257.023565\n",
      "\n",
      "[393 rows x 2 columns]\n",
      "bute\n",
      "evi_n\n",
      "jervis\n",
      "evi_s\n",
      "howe\n",
      "jdf\n",
      "skagit\n",
      "puget\n",
      "toba\n",
      "fraser\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-89bc31a10411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0msubarea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfraserratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0mflux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nonFraser'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nonFraser'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Daily Flow'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                     \u001b[0msubarea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfraserratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ocean/sallen/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_fallback_to_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "flows = {}\n",
    "for name in names:\n",
    "    print (name)\n",
    "    if rivers_for_watershed[name]['secondary'] == 'False':\n",
    "        flows[name] = do_a_pair(name, watershed_from_river, startdate, enddate, \n",
    "                                rivers_for_watershed[name]['primary'], False)\n",
    "    elif name == 'fraser':\n",
    "        flows['Fraser'], flows['nonFraser'] = do_fraser(watershed_from_river, startdate, enddate,\n",
    "                               rivers_for_watershed[name]['primary'],\n",
    "                               rivers_for_watershed[name]['secondary'])\n",
    "        print (flows['nonFraser'][flows['nonFraser'].index >= dt.datetime(2018,1,1)])\n",
    "    else:\n",
    "        flows[name] = do_a_pair(name, watershed_from_river, startdate, enddate,\n",
    "                                rivers_for_watershed[name]['primary'], True,\n",
    "                               rivers_for_watershed[name]['secondary'])\n",
    "\n",
    "for day in arrow.Arrow.range('day', startdate, enddate):\n",
    "    runoff = np.zeros((horz_area.shape[0], horz_area.shape[1]))\n",
    "    run_depth = np.ones_like(runoff)\n",
    "    run_temp = np.empty_like(runoff)\n",
    "    for name in names:\n",
    "        print (name)\n",
    "        if name == 'fraser':\n",
    "            for key in prop_dict[name]:\n",
    "                if \"Fraser\" in key:\n",
    "                    flux = flows['Fraser'][flows['Fraser'].index == day.naive]['Daily Flow'][0]\n",
    "                    subarea = fraserratio\n",
    "                else:\n",
    "                    flux = flows['nonFraser'][flows['nonFraser'].index == day.naive]['Daily Flow'][0]\n",
    "                    subarea = 1 - fraserratio\n",
    "        \n",
    "                river = prop_dict['fraser'][key]\n",
    "                runoff = rivertools.fill_runoff_array(flux*river['prop']/subarea, river['i'],\n",
    "                          river['di'], river['j'], river['dj'], river['depth'], runoff, \n",
    "                          run_depth, horz_area)[0]\n",
    "        else:\n",
    "            flowtoday = flows[name][flows[name].index == day.naive]['Daily Flow'][0]\n",
    "            runoff, run_depth, run_temp = rivertools.put_watershed_into_runoff('constant', horz_area,\n",
    "                                            flowtoday, runoff, run_depth, run_temp,\n",
    "                                            prop_dict[name])\n",
    "            \n",
    "    write_file(day, runoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy = xr.open_dataset('../../../grid/bathymetry_201702.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imin, imax = 0, 898\n",
    "jmin, jmax = 0, 394\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 9))\n",
    "ax.contourf(bathy.Bathymetry[imin:imax, jmin:jmax], cmap='winter_r')\n",
    "for i in range(imin, imax):\n",
    "    for j in range(jmin, jmax):\n",
    "        flux = runoff[i, j]\n",
    "        if flux > 0 :\n",
    "            plt.scatter(j - jmin, i - imin, s=flux*1000, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readitin = xr.open_dataset('ncfiles/R202102Dailies_y2017m03d31.nc')\n",
    "climate = xr.open_dataset('ncfiles/R202101Dailies_y2017m03d31.nc')\n",
    "fluxarray = readitin.rorunoff[0]\n",
    "climatearray = climate.rorunoff[0]\n",
    "(fluxarray - climatearray).plot(cmap='Reds');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readitin = xr.open_dataset('ncfiles/R202102Dailies_y2017m03d31.nc')\n",
    "climate = xr.open_dataset('/results/forcing/rivers/R201702DFraCElse_y2017m03d31.nc')\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 9))\n",
    "for ax in axs:\n",
    "    ax.contourf(bathy.Bathymetry[imin:imax, jmin:jmax], cmap='winter_r')\n",
    "for i in range(imin, imax):\n",
    "    for j in range(jmin, jmax):\n",
    "        flux = fluxarray[i, j]\n",
    "        if flux > 0 :\n",
    "            axs[1].scatter(j - jmin, i - imin, s=flux*1000, color='r')\n",
    "            axs[0].scatter(j - jmin, i - imin, s=climatearray[i, j]*1000, color='r')\n",
    "            if flux > climatearray[i, j]:\n",
    "                axs[2].scatter(j - jmin, i-imin, s=(flux-climatearray[i, j])*1000, color='r')\n",
    "            else:\n",
    "                axs[2].scatter(j - jmin, i-imin, s=(climatearray[i, j]-flux)*1000, color='b')\n",
    "            \n",
    "readitin.close()\n",
    "climate.close()\n",
    "fig.suptitle('March 31, 2017');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readitin = xr.open_dataset('ncfiles/R202102Dailies_y2017m05d31.nc')\n",
    "climate = xr.open_dataset('/results/forcing/rivers/R201702DFraCElse_y2017m05d31.nc')\n",
    "fluxarray = readitin.rorunoff[0]\n",
    "climatearray = climate.rorunoff[0]\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 9))\n",
    "fig.suptitle('May 31, 2014')\n",
    "for ax in axs:\n",
    "    ax.contourf(bathy.Bathymetry[imin:imax, jmin:jmax], cmap='winter_r')\n",
    "for i in range(imin, imax):\n",
    "    for j in range(jmin, jmax):\n",
    "        flux = fluxarray[i, j]\n",
    "        if flux > 0 :\n",
    "            axs[1].scatter(j - jmin, i - imin, s=flux*1000, color='r')\n",
    "            axs[0].scatter(j - jmin, i - imin, s=climatearray[i, j]*1000, color='r')\n",
    "            if flux > climatearray[i, j]:\n",
    "                axs[2].scatter(j - jmin, i-imin, s=(flux-climatearray[i, j])*1000, color='r')\n",
    "            else:\n",
    "                axs[2].scatter(j - jmin, i-imin, s=(climatearray[i, j]-flux)*1000, color='b')\n",
    "            \n",
    "readitin.close()\n",
    "climate.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readitin = xr.open_dataset('ncfiles/R202102Dailies_y2017m08d31.nc')\n",
    "climate = xr.open_dataset('/results/forcing/rivers/R201702DFraCElse_y2017m08d31.nc')\n",
    "fluxarray = readitin.rorunoff[0]\n",
    "climatearray = climate.rorunoff[0]\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 9))\n",
    "fig.suptitle('August 31, 2017')\n",
    "for ax in axs:\n",
    "    ax.contourf(bathy.Bathymetry[imin:imax, jmin:jmax], cmap='winter_r')\n",
    "for i in range(imin, imax):\n",
    "    for j in range(jmin, jmax):\n",
    "        flux = fluxarray[i, j]\n",
    "        if flux > 0 :\n",
    "            axs[1].scatter(j - jmin, i - imin, s=flux*1000, color='r')\n",
    "            axs[0].scatter(j - jmin, i - imin, s=climatearray[i, j]*1000, color='r')\n",
    "            if flux > climatearray[i, j]:\n",
    "                axs[2].scatter(j - jmin, i-imin, s=(flux-climatearray[i, j])*1000, color='r')\n",
    "            else:\n",
    "                axs[2].scatter(j - jmin, i-imin, s=(climatearray[i, j]-flux)*1000, color='b')\n",
    "            \n",
    "readitin.close()\n",
    "climate.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readitin = xr.open_dataset('ncfiles/R202102Dailies_y2017m12d31.nc')\n",
    "climate = xr.open_dataset('/results/forcing/rivers/R201702DFraCElse_y2017m12d31.nc')\n",
    "fluxarray = readitin.rorunoff[0]\n",
    "climatearray = climate.rorunoff[0]\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 9))\n",
    "fig.suptitle('December 31, 2017')\n",
    "for ax in axs:\n",
    "    ax.contourf(bathy.Bathymetry[imin:imax, jmin:jmax], cmap='winter_r')\n",
    "for i in range(imin, imax):\n",
    "    for j in range(jmin, jmax):\n",
    "        flux = fluxarray[i, j]\n",
    "        if flux > 0 :\n",
    "            axs[1].scatter(j - jmin, i - imin, s=flux*1000, color='r')\n",
    "            axs[0].scatter(j - jmin, i - imin, s=climatearray[i, j]*1000, color='r')\n",
    "            if flux > climatearray[i, j]:\n",
    "                axs[2].scatter(j - jmin, i-imin, s=(flux-climatearray[i, j])*1000, color='r')\n",
    "            else:\n",
    "                axs[2].scatter(j - jmin, i-imin, s=(climatearray[i, j]-flux)*1000, color='b')\n",
    "            \n",
    "readitin.close()\n",
    "climate.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2014: All blue, all the time?  Not quite, PS is red is December and May."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readitin = xr.open_dataset('ncfiles/R202102Dailies_y2014m11d01.nc')\n",
    "climate = xr.open_dataset('/results/forcing/rivers/R201702DFraCElse_y2014m11d01.nc')\n",
    "fluxarray = readitin.rorunoff[0]\n",
    "climatearray = climate.rorunoff[0]\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 9))\n",
    "fig.suptitle('November 1, 2014')\n",
    "for ax in axs:\n",
    "    ax.contourf(bathy.Bathymetry[imin:imax, jmin:jmax], cmap='winter_r')\n",
    "for i in range(imin, imax):\n",
    "    for j in range(jmin, jmax):\n",
    "        flux = fluxarray[i, j]\n",
    "        if flux > 0 :\n",
    "            axs[1].scatter(j - jmin, i - imin, s=flux*1000, color='r')\n",
    "            axs[0].scatter(j - jmin, i - imin, s=climatearray[i, j]*1000, color='r')\n",
    "            if flux > climatearray[i, j]:\n",
    "                axs[2].scatter(j - jmin, i-imin, s=(flux-climatearray[i, j])*1000, color='r')\n",
    "            else:\n",
    "                axs[2].scatter(j - jmin, i-imin, s=(climatearray[i, j]-flux)*1000, color='b')\n",
    "            \n",
    "readitin.close()\n",
    "climate.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
